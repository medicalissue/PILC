import torch
from bridge_text_encoder import BridgeTextEncoder, get_tokenizer

def test_bridge_encoder():
    print("=== Bridge Text Encoder í…ŒìŠ¤íŠ¸ (32ì°¨ì›) ===")
    
    # 1. í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    tokenizer = get_tokenizer()
    texts = [
        "A red sports car",
        "A beautiful cat sitting on a table", 
        "Sunset over mountains with orange sky",
        "A dog playing in the park"
    ]
    
    tokens = tokenizer(
        texts,
        max_length=77,
        padding=True,
        truncation=True,
        return_tensors="pt"
    )
    
    print(f"Input texts: {len(texts)}")
    print(f"Tokenized shape: {tokens['input_ids'].shape}")  # [4, 77]
    print(f"Attention mask shape: {tokens['attention_mask'].shape}")  # [4, 77]
    
    # 2. Bridge Encoder í…ŒìŠ¤íŠ¸ (32ì°¨ì›!)
    bridge_encoder = BridgeTextEncoder(
        num_latent_tokens=64,
        embed_dim=32,  # â† 32ì°¨ì›ìœ¼ë¡œ ë³€ê²½!
        freeze_clip=True
    )
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
    trainable_params = sum(p.numel() for p in bridge_encoder.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in bridge_encoder.parameters())
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Total parameters: {total_params:,}")
    print(f"CLIP frozen: {not any(p.requires_grad for p in bridge_encoder.clip_text.parameters())}")
    
    # 3. Forward pass
    print("\n=== Forward Pass í…ŒìŠ¤íŠ¸ ===")
    with torch.no_grad():
        output, attention = bridge_encoder(
            tokens['input_ids'],
            tokens['attention_mask'],
            return_attention=True
        )
    
    print(f"âœ… Output shape: {output.shape}")  # [4, 64, 32]
    print(f"âœ… Attention shape: {attention.shape}")  # [4, 64, 77]
    
    # 4. ì°¨ì› ê²€ì¦
    batch_size, num_tokens, embed_dim = output.shape
    assert batch_size == len(texts), f"Batch size mismatch: {batch_size} != {len(texts)}"
    assert num_tokens == 64, f"Token count mismatch: {num_tokens} != 64"
    assert embed_dim == 32, f"Embedding dim mismatch: {embed_dim} != 32"
    print("âœ… ëª¨ë“  ì°¨ì› ê²€ì¦ í†µê³¼!")
    
    # 5. ì¶œë ¥ ê°’ ë²”ìœ„ í™•ì¸
    output_mean = output.mean().item()
    output_std = output.std().item()
    output_min = output.min().item()
    output_max = output.max().item()
    
    print(f"\n=== ì¶œë ¥ í†µê³„ ===")
    print(f"Mean: {output_mean:.4f}")
    print(f"Std: {output_std:.4f}")
    print(f"Min: {output_min:.4f}")
    print(f"Max: {output_max:.4f}")
    
    # 6. Attention ë¶„ì„ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)
    print(f"\n=== Attention ë¶„ì„ ===")
    analyze_attention(texts[0], tokens['input_ids'][0], attention[0], tokenizer)
    
    # 7. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ê¸¸ì´ í…ŒìŠ¤íŠ¸
    print(f"\n=== ë‹¤ì–‘í•œ ê¸¸ì´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ ===")
    test_various_lengths(bridge_encoder, tokenizer)
    
    print("\nğŸ‰ Bridge Text Encoder í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    return bridge_encoder, tokenizer

def analyze_attention(text, token_ids, attention_weights, tokenizer):
    """Attention pattern ìƒì„¸ ë¶„ì„"""
    tokens_list = tokenizer.convert_ids_to_tokens(token_ids)
    
    print(f"ë¶„ì„ í…ìŠ¤íŠ¸: '{text}'")
    print(f"í† í° ë¦¬ìŠ¤íŠ¸: {tokens_list[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ
    
    # ì²˜ìŒ 5ê°œ query ë¶„ì„
    for q in range(5):
        top_attention = attention_weights[q].topk(3)
        attended_info = []
        
        for idx, score in zip(top_attention.indices, top_attention.values):
            if idx < len(tokens_list):
                token = tokens_list[idx]
                attended_info.append(f"{token}({score:.3f})")
        
        print(f"  Query {q}: {attended_info}")
        
        # Query ì—­í•  ì¶”ì •
        if q == 0:
            print(f"    â†’ ì¶”ì • ì—­í• : ì£¼ìš” ê°ì²´/ê°œë… ì‹ë³„")
        elif q == 1:
            print(f"    â†’ ì¶”ì • ì—­í• : ì†ì„±/ìˆ˜ì‹ì–´ ì •ë³´")
        elif q == 2:
            print(f"    â†’ ì¶”ì • ì—­í• : ê³µê°„/ë§¥ë½ ì •ë³´")

def test_various_lengths(model, tokenizer):
    """ë‹¤ì–‘í•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸"""
    test_texts = [
        "Cat",  # ë§¤ìš° ì§§ìŒ
        "A red car driving fast on the highway",  # ì¤‘ê°„
        "A beautiful sunset over the mountains with orange and pink clouds reflecting on the calm lake water",  # ê¸´ í…ìŠ¤íŠ¸
        "",  # ë¹ˆ ë¬¸ìì—´
    ]
    
    for i, text in enumerate(test_texts):
        if text == "":
            text = "[EMPTY]"
            test_text = ""
        else:
            test_text = text
            
        tokens = tokenizer(
            [test_text], 
            max_length=77, 
            padding=True, 
            truncation=True, 
            return_tensors="pt"
        )
        
        with torch.no_grad():
            output = model(tokens['input_ids'], tokens['attention_mask'])
        
        print(f"  Text {i+1} ('{text[:20]}...'): {output.shape} âœ…")

if __name__ == "__main__":
    test_bridge_encoder()